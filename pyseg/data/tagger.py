# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Wmm-4G31z5hLGzujFFnCaqFqoTi0bdi
"""

import pickle
import io
from google.colab import files
import jieba
import jieba.posseg as pseg

df = open('corpus.pkl','rb')
train=pickle.load(df)
df.close()

root_check_dict = {
    "n": "名词",
    "nr": "人名",
    "nr1": "汉语姓氏",
    "nr2": "汉语名字",
    "nrj": "日语人名",
    "nrf": "音译人名",
    "ns": "地名",
    "nsf": "音译地名",
    "nt": "机构团体名",
    "nz": "其它专名",
    "nl": "名词性惯用语",
    "ng": "名词性语素",
    "t": "时间词",
    "tg": "时间词性语素",
    "s": "处所词",
    "f": "方位词",
    "v": "动词",
    "vd": "副动词",
    "vn": "名动词",
    "vshi": "动词“是”",
    "vyou": "动词“有”",
    "vf": "趋向动词",
    "vx": "形式动词",
    "vi": "不及物动词（内动词）",
    "vl": "动词性惯用语",
    "vg": "动词性语素",
    "a": "形容词",
    "ad": "副形词",
    "an": "名形词",
    "ag": "形容词性语素",
    "al": "形容词性惯用语",
    "b": "区别词",
    "bl": "区别词性惯用语",
    "z": "状态词",
    "r": "代词",
    "rr": "人称代词",
    "rz": "指示代词",
    "rzt": "时间指示代词",
    "rzs": "处所指示代词",
    "rzv": "谓词性指示代词",
    "ry": "疑问代词",
    "ryt": "时间疑问代词",
    "rys": "处所疑问代词",
    "ryv": "谓词性疑问代词",
    "rg": "代词性语素",
    "m": "数词",
    "mq": "数量词",
    "q": "量词",
    "qv": "动量词",
    "qt": "时量词",
    "d": "副词",
    "p": "介词",
    "pba": "介词“把”",
    "pbei": "介词“被”",
    "c": "连词",
    "cc": "并列连词",
    "u": "助词",
    "uzhe": "着",
    "ule": "了 喽",
    "uguo": "过",
    "ude1": "的 底",
    "ude2": "地",
    "ude3": "得",
    "usuo": "所",
    "udeng": "等 等等 云云",
    "uyy": "一样 一般 似的 般",
    "udh": "的话",
    "uls": "来讲 来说 而言 说来",
    "uzhi": "之",
    "ulian": "连",
    "e": "叹词",
    "y": "语气词(delete yg)",
    "o": "拟声词",
    "h": "前缀",
    "k": "后缀",
    "x": "字符串",
    "xx": "非语素字",
    "xu": "网址URL",
    "w": "标点符号",
    "wkz": "左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( [ { <",
    "wky": "右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) ] { >",
    "wyz": "左引号，全角：“ ‘ 『",
    "wyy": "右引号，全角：” ’ 』",
    "wj": "句号，全角：。",
    "ww": "问号，全角：？ 半角：?",
    "wt": "叹号，全角：！ 半角：!",
    "wd": "逗号，全角：， 半角：,",
    "wf": "分号，全角：； 半角： ; ",
    "wn": "顿号，全角：、",
    "wm": "冒号，全角：： 半角： :",
    "ws": "省略号，全角：…… …",
    "wp": "破折号，全角：—— －－ ——－ 半角：—",
    "wb": "百分号千分号，全角：％ ‰ 半角：%",
    "wh": "单位符号，全角：￥ ＄ ￡ ° ℃ 半角 $"
}

training_set=[]

for item in train:
  words = pseg.cut(item[0][0])
  tmp_word_list=[]
  tmp_tag_list=[]
  for word,tag in words:
    tmp_word_list.append(word)
    tmp_tag_list.append(tag)
  training_set.append((tmp_word_list,tmp_tag_list))

word_to_ix={}
for sent,tag in training_set:
    for word in sent:
        if word not in word_to_ix:
            word_to_ix[word]=len(word_to_ix)
print(word_to_ix)

word_to_ix["None"]=len(word_to_ix)

tag_to_ix={}
for sent,tag in training_set:
    for word in tag:
        if word not in tag_to_ix:
            tag_to_ix[word]=len(tag_to_ix)
print(tag_to_ix)

ix_to_tag={}
for key,value in tag_to_ix.items():
  ix_to_tag[value]=key

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

EMBEDDING_DIM=50
HIDDEN_DIM=50

class LSTMTagger(nn.Module):
    def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size):
        super(LSTMTagger,self).__init__()
        self.hidden_dim=hidden_dim
        self.word_embeddings=nn.Embedding(vocab_size,embedding_dim)
        self.lstm=nn.LSTM(embedding_dim,hidden_dim)
        self.hidden2tag=nn.Linear(hidden_dim,tagset_size)
        self.hidden=self.init_hidden()
    def init_hidden(self):
        #the axes semantics are (num_layers,minibatch_size,hidden_size)
        return (torch.zeros(1,1,self.hidden_dim),
                (torch.zeros(1,1,self.hidden_dim)))
    def forward(self, sentence):
        embeds=self.word_embeddings(sentence)
        lstm_out,self.hidden=self.lstm(embeds.view(len(sentence),1,-1),self.hidden)
        tag_space=self.hidden2tag(lstm_out.view(len(sentence),-1))
        tag_scores=F.log_softmax(tag_space,dim=1)
        return tag_scores

model=LSTMTagger(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))
loss_function=nn.NLLLoss()
optimizer=optim.SGD(model.parameters(),lr=0.1)

def prepare_sequence(seq,to_ix):
    idxs=[]
    for w in seq:
      if(w in to_ix.keys()):
        idxs.append(to_ix[w])
      else:
        idxs.append(to_ix["None"])
    return torch.tensor(idxs,dtype=torch.long)

model=LSTMTagger(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))
loss_function=nn.NLLLoss()
optimizer=optim.SGD(model.parameters(),lr=0.1)
#before training
# with torch.no_grad():
#     inputs=prepare_sequence(training_data[0][0],word_to_ix)
#     tag_scores=model(inputs)
#     print(tag_scores)
 
for epoch in range(20):
    for sentence,tags in training_set:
        model.zero_grad()
        model.hidden=model.init_hidden()
        sentence_in=prepare_sequence(sentence,word_to_ix)
        targets=prepare_sequence(tags,tag_to_ix)
        tag_scores=model(sentence_in)
        loss=loss_function(tag_scores,targets)
        loss.backward()
        optimizer.step()
    print(epoch)
# after training
# with torch.no_grad():
#     inputs=prepare_sequence(training_set[0][0],word_to_ix)
#     tag_scores=model(inputs)
#     print(tag_scores.argmax(dim=1).numpy().tolist())

with torch.no_grad():
    inputs=prepare_sequence(training_set[273][0],word_to_ix)
    tag_scores=model(inputs)
    tem=tag_scores.argmax(dim=1).numpy().tolist()
    sen=training_set[273][0]

for idx in range(len(sen)):
  print(sen[idx],end="/")
  print(ix_to_tag[tem[idx]])

torch.save(model,"partspeech.pkl")

model=torch.load("partspeech.pkl")

with open('extra_dict.pkl', 'wb') as outp:
	pickle.dump(word_to_ix, outp)
	pickle.dump(tag_to_ix, outp)
	pickle.dump(ix_to_tag, outp)
print('** Finished saving the data.')

training_set[273][0]

